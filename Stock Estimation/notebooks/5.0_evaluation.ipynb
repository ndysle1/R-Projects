{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Assessment of data mining results with respect to success criteria</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   1. In the first stage of this project, I identified the success criteria as follows:<br>\n",
    "The biggest success criteria that is shared across all of the projectâ€™s objectives is learning valuable skills from this training. I believe this project will be a success if I come away from this project with more confidence in my abilities, an increased technical skillset, and/or a better understanding of a data science work environment. However, specific success critieria for each project objective will be defined in the following:\n",
    "    - Project Understanding: Success will be gaining a deeper understanding of the project and its purpose and throughly answering all questions asked. \n",
    "    - Data Understanding: Success will be learning more about the variables in the dataset and understanding more about how they can be leveraged in future analysis. I should have a clear plan for the data preparation stage once this stage is complete. \n",
    "    - Data Preparation: Success will be identifying outliers, missing observations, and delivering a dataset that is throroughly prepared for analysis. If I was successful in this stage, then I will not run into any major data related issues when modeling. \n",
    "    - Modeling: Success will be creating a model that accurately predicts whether a stock should be sold or bought.\n",
    "    - Evaluation: Success will be determined using the AUC of the ROC curve. \n",
    "<br>\n",
    "<br>\n",
    "   2. After completing the project, I believe that the project already meets the initial objectives. After the completion of notebook 4, the new skills and increased familiarity I have with machine learning, GitHub, and other programs is overwhelming. In addition, the final model that I created in notebook 4 did well in terms of its predicting power despite the challenges that were inherent in the provided dataset and its relationship with the stock market. Individual stocks and the overall stock market are historically difficult to predict. The final results for the project were as follows:\n",
    "       - **Baseline:** ACC - .621 , F1 - .520 , AUC - .612\n",
    "       - **Final:** ACC - .636 , F1 - .556 , AUC - .633 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Review of Process</b>\n",
    "- Summarizing the process and highlighting activities that have been missed and those that should be repeated. </br>\n",
    "    - **Factor Review:** All the steps of the CRISP-DM model were implemented according to the objectives set out in the beginning of this training. <br>\n",
    "     - **Data Exploration:**\n",
    "       - Variable Names: \n",
    "         - The \"year\" variable was created and all variables between the datasets were renamed to match. All data sets were then combined into one data set. \n",
    "         - The main changes made to the final dataset was the removal of 4 extreme outliers detected through Cook's Distance, variable name changes, and the removal of highly collinear or zero variance variables.\n",
    "     - **Data Preparation:**\n",
    "       - Duplicates:\n",
    "         - Columns with duplicate names were removed. Then column sums were used to find similar or identical variables that had slightly different names but were still duplicates. Finally, these variables were merged to create a single variable.\n",
    "       - Variable Names: \n",
    "         - All variables were converted to lower case and a variety of steps were taken to make them uniform. \n",
    "       - Categorical Encoding:\n",
    "         - \"sector\" was changed to a number \"sector_num\" to ease the machine learning process.\n",
    "       - Missing Data:\n",
    "         - Removed those rows with a sum of missing data greater than 50.\n",
    "         - Removed those columns with missing data greater than 15%.\n",
    "     - **Data Modeling:**\n",
    "       - Base Model: \n",
    "         - After trying various models, the random forest model was chosen as the best base model. \n",
    "         - Data was split according to a 70/30 ratio with \"year\" being removed so that the model would not rely so heavily on it for classification.\n",
    "       - Model Improvement:\n",
    "         - The dataset was scaled using using the standard deviation and imputed using knn imputation.\n",
    "       - Feature Selection: \n",
    "         - Decision trees were run for each year to see the importance of different variables. All variables that had an importance greater than 0 were chosen and combined in a reduced data set. This dataset led to worse results than the full dataset when used on the random forest model so the decision was made to keep the full dataset moving forward.\n",
    "       - Hypertuning:\n",
    "         - The mtry value was tuned to find the optimal value.\n",
    "       - Additional Tuning: \n",
    "         - The ntree value was also expanded to included larger values than 500. The results showed that 500 was still the best value to choose.\n",
    "         - Controls were selected and repeated cross-validation was implemented in the final model. This included the value of 500 for ntree, but upon using the previous mtry value results were sub-optimal. It was decided to let the cross-validation model choose its own optimal mtry.\n",
    "    - **Model Review:** Using only the included data, I believe that the model was built correctly. The code for the final model is listed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code from the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ntree=500\n",
    "control <- trainControl(method=\"repeatedcv\", repeats = 2, number=2, search=\"random\")\n",
    "set.seed(123)\n",
    "fit.rf <- train(class~., data=train, method=\"rf\", ntree = ntree)\n",
    "fit.rf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. List of possible actions</b>\n",
    "- Identify additional datasets to improve the model's performance: The benefits of this are discussed in the decision section below.\n",
    "- Deploy the model as-is: This would allow for a reasonably accurate model to be utilized but it still may not be the best possible model.\n",
    "- Further tune the model's parameters: Additional tuning of the parameters could increase the model's performance but only ever so slightly. The amount of time and effort does not lead to great enough returns to go down this route.\n",
    "- Select new type of model: It may be possible to increase the performance of the model by selecting a different technique.\n",
    "\n",
    "<b>4. Decision</b>\n",
    "- I feel that the current model does a satisfactory job at predicting which stocks I should buy or sell. However, I do not feel comfortable using this model in its current state. Due to huge impact certain years have on the returns of stocks, I think that any model that is attempting to predict whether stocks should be sold or not should have larger macroeconomic variables included. A company's financial records are not going to be able to predict a recession or shock to the stock market in most instances. Accurate economic forecasts, however, could be included with additional years of financial information for stocks so that better predictions can be made. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
